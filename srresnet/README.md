# SRCNN系列

训练数据为91-image，测试数据为Set5

评价指标为PSNR

实验环境：Ubuntu20.04 LTS、pytorch 1.6.0、CPU

超参数:

1. 学习率：余弦退火、`lr = 1e-3`
2. `batch_size = 32`
3. `adam`

## SRCNN及其衍生网络介绍

SRCNN的整体思路如下:

1. 对于低质量图像，先做双立方插值，将其大小resize成高质量图像大小

2. 对于resize结果，经过一个三层网络，得到重建后结果

3. loss原文中采用MSELoss()，即L2Loss


网络结构如下:

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [64, 64, 63, 47]           5,248
              ReLU-2           [64, 64, 63, 47]               0
            Conv2d-3           [64, 32, 63, 47]          51,232
              ReLU-4           [64, 32, 63, 47]               0
            Conv2d-5            [64, 1, 63, 47]             801
================================================================
Total params: 57,281
Trainable params: 57,281
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.72
Forward/backward pass size (MB): 279.04
Params size (MB): 0.22
Estimated Total Size (MB): 279.98
----------------------------------------------------------------

```

网络结果如下：

```json
Loss: MSELoss()

epoch: 1/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:42<00:00, 153.34it/s, loss=0.013516]
eval psnr: 31.18
epoch: 2/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:44<00:00, 104.83it/s, loss=0.001455]
eval psnr: 32.73
epoch: 3/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:43<00:00, 134.09it/s, loss=0.001149]
eval psnr: 33.44
epoch: 4/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:47<00:00, 147.26it/s, loss=0.001019]
eval psnr: 33.90
epoch: 5/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:44<00:00, 140.56it/s, loss=0.000951]
eval psnr: 34.18
epoch: 6/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:50<00:00, 152.17it/s, loss=0.000912]
eval psnr: 34.33
epoch: 7/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:48<00:00, 150.96it/s, loss=0.000891]
eval psnr: 34.42
epoch: 8/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:43<00:00, 142.74it/s, loss=0.000879]
eval psnr: 34.47
epoch: 9/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:49<00:00, 147.84it/s, loss=0.000872]
eval psnr: 34.49
epoch: 10/10: 100%|██████████████████████████████████████████████| 21888/21888 [02:47<00:00, 148.50it/s, loss=0.000869]
eval psnr: 34.50
```


FSRCNN的整体思路如下：

1. SRCNN网络隐藏层channel较大，我们可以修改中间层channel，损失部分精度换取极高速度

2. 加深SRCNN网络

网络结构如下：

> 需要注意的是，我采用的数据集是SRCNN的数据集，SRCNN一开始是需要经过一个传统图像处理resize的，其数据集为相同大小图像之间的生成，所以这里ConvTranspose2d没有加stride

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [64, 56, 63, 47]           1,456
             PReLU-2           [64, 56, 63, 47]              56
            Conv2d-3           [64, 12, 63, 47]             684
             PReLU-4           [64, 12, 63, 47]              12
            Conv2d-5           [64, 12, 63, 47]           1,308
             PReLU-6           [64, 12, 63, 47]              12
            Conv2d-7           [64, 12, 63, 47]           1,308
             PReLU-8           [64, 12, 63, 47]              12
            Conv2d-9           [64, 12, 63, 47]           1,308
            PReLU-10           [64, 12, 63, 47]              12
           Conv2d-11           [64, 12, 63, 47]           1,308
            PReLU-12           [64, 12, 63, 47]              12
           Conv2d-13           [64, 56, 63, 47]             728
            PReLU-14           [64, 56, 63, 47]              56
  ConvTranspose2d-15            [64, 1, 63, 47]           4,537
================================================================
Total params: 12,809
Trainable params: 12,809
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.72
Forward/backward pass size (MB): 498.80
Params size (MB): 0.05
Estimated Total Size (MB): 499.57
----------------------------------------------------------------

```

网络结果如下：

```json
Loss: MSELoss()

epoch: 1/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:37<00:00, 240.16it/s, loss=0.265750]
eval psnr: 16.52
epoch: 2/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:40<00:00, 258.64it/s, loss=0.028631]
eval psnr: 20.57
epoch: 3/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:36<00:00, 273.97it/s, loss=0.015406]
eval psnr: 22.78
epoch: 4/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:46<00:00, 258.63it/s, loss=0.010839]
eval psnr: 24.21
epoch: 5/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:47<00:00, 239.36it/s, loss=0.008628]
eval psnr: 25.17
epoch: 6/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:44<00:00, 230.92it/s, loss=0.007374]
eval psnr: 25.81
epoch: 7/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:47<00:00, 260.66it/s, loss=0.006631]
eval psnr: 26.23
epoch: 8/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:37<00:00, 293.19it/s, loss=0.006187]
eval psnr: 26.48
epoch: 9/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:33<00:00, 274.06it/s, loss=0.005948]
eval psnr: 26.61
epoch: 10/10: 100%|██████████████████████████████████████████████| 21888/21888 [01:46<00:00, 221.99it/s, loss=0.005846]
eval psnr: 26.64
```

从上述结果可知，FSRCNN牺牲一定的精度换取速度

>   精度损失很大，使用L1Loss可以修复该问题，详细见下


## 对比实验

考虑到以下对比：

1. L1Loss与MESLoss的比较

2. 是否增加BatchNorm或者InstanceNorm等归一化手段

3. 调整FSRCNN中间层channel，观察精度

### loss比较

我们将loss调整为L1Loss，观察实验现象

```json
Loss: L1Loss()

epoch: 1/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:41<00:00, 157.22it/s, loss=0.037541]
eval psnr: 32.93
epoch: 2/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:41<00:00, 140.48it/s, loss=0.019021]
eval psnr: 33.80
epoch: 3/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:32<00:00, 182.93it/s, loss=0.017379]
eval psnr: 34.18
epoch: 4/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:41<00:00, 143.05it/s, loss=0.016600]
eval psnr: 34.59
epoch: 5/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:43<00:00, 148.73it/s, loss=0.016093]
eval psnr: 34.73
epoch: 6/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:42<00:00, 143.48it/s, loss=0.015804]
eval psnr: 34.81
epoch: 7/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:45<00:00, 152.13it/s, loss=0.015632]
eval psnr: 34.87
epoch: 8/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:45<00:00, 154.39it/s, loss=0.015514]
eval psnr: 34.92
epoch: 9/10: 100%|███████████████████████████████████████████████| 21888/21888 [02:42<00:00, 156.12it/s, loss=0.015448]
eval psnr: 34.94
epoch: 10/10: 100%|██████████████████████████████████████████████| 21888/21888 [02:46<00:00, 123.65it/s, loss=0.015417]
eval psnr: 34.94
```

可见，在SRCNN上，使用L1Loss会使PSNR指标略有提升

```json
Loss: L1Loss()

epoch: 1/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:46<00:00, 260.71it/s, loss=0.105976]
eval psnr: 25.23
epoch: 2/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:45<00:00, 259.96it/s, loss=0.043584]
eval psnr: 28.72
epoch: 3/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:43<00:00, 216.37it/s, loss=0.034260]
eval psnr: 30.44
epoch: 4/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:41<00:00, 224.40it/s, loss=0.030300]
eval psnr: 31.21
epoch: 5/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:43<00:00, 289.73it/s, loss=0.028138]
eval psnr: 31.74
epoch: 6/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:34<00:00, 273.23it/s, loss=0.026745]
eval psnr: 32.07
epoch: 7/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:46<00:00, 248.36it/s, loss=0.025838]
eval psnr: 32.28
epoch: 8/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:49<00:00, 235.63it/s, loss=0.025262]
eval psnr: 32.40
epoch: 9/10: 100%|███████████████████████████████████████████████| 21888/21888 [01:53<00:00, 244.13it/s, loss=0.024931]
eval psnr: 32.49
epoch: 10/10: 100%|██████████████████████████████████████████████| 21888/21888 [01:35<00:00, 272.00it/s, loss=0.024790]
eval psnr: 32.51
```

可见，在FSRCNN上，使用L1Loss会使得PSNR指标有很大提升（即使从数学上，L2Loss更适合刷高PSNR）

解释：

1.  L1Loss收敛性能更优
2.  L2Loss忽略了图像内容本身

>   参考文章[《Loss Functions for Neural Networks for Image Processing》](https://arxiv.org/abs/1511.08861)

### 归一化比较

我们使用InstanceNorm（SRResNet中使用IN，而非BN）

网络结构如下：

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [64, 64, 63, 47]           1,664
             PReLU-2           [64, 64, 63, 47]              64
            Conv2d-3           [64, 32, 63, 47]           2,080
             PReLU-4           [64, 32, 63, 47]              32
            Conv2d-5           [64, 32, 63, 47]           1,056
    InstanceNorm2d-6           [64, 32, 63, 47]              64
             PReLU-7           [64, 32, 63, 47]              32
            Conv2d-8           [64, 32, 63, 47]           1,056
    InstanceNorm2d-9           [64, 32, 63, 47]              64
            PReLU-10           [64, 32, 63, 47]              32
           Conv2d-11           [64, 32, 63, 47]           1,056
   InstanceNorm2d-12           [64, 32, 63, 47]              64
            PReLU-13           [64, 32, 63, 47]              32
           Conv2d-14           [64, 32, 63, 47]           1,056
   InstanceNorm2d-15           [64, 32, 63, 47]              64
            PReLU-16           [64, 32, 63, 47]              32
           Conv2d-17           [64, 64, 63, 47]           2,112
            PReLU-18           [64, 64, 63, 47]              64
  ConvTranspose2d-19            [64, 1, 63, 47]           5,185
================================================================
Total params: 15,809
Trainable params: 15,809
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.72
Forward/backward pass size (MB): 1019.29
Params size (MB): 0.06
Estimated Total Size (MB): 1020.07
----------------------------------------------------------------

```

网络结果如下：

```json
Loss: MSELoss()

epoch: 1/5: 100%|████████████████████████████████████████████████| 21888/21888 [02:48<00:00, 133.48it/s, loss=0.780161]
eval psnr: 8.53
epoch: 2/5: 100%|████████████████████████████████████████████████| 21888/21888 [02:55<00:00, 152.24it/s, loss=0.146877]
eval psnr: 9.34
epoch: 3/5: 100%|████████████████████████████████████████████████| 21888/21888 [02:44<00:00, 152.47it/s, loss=0.091404]
eval psnr: 9.66
epoch: 4/5: 100%|████████████████████████████████████████████████| 21888/21888 [02:43<00:00, 145.06it/s, loss=0.071915]
eval psnr: 9.94
epoch: 5/5: 100%|████████████████████████████████████████████████| 21888/21888 [02:41<00:00, 150.35it/s, loss=0.064856]
eval psnr: 10.10
```

完全没用（可以加速训练），甚至很拉跨

### FSRCNN微调

修改部分FSRCNN中间层的channel

网络结构如下：

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [64, 64, 63, 47]           1,664
             PReLU-2           [64, 64, 63, 47]              64
            Conv2d-3           [64, 32, 63, 47]           2,080
             PReLU-4           [64, 32, 63, 47]              32
            Conv2d-5           [64, 32, 63, 47]           9,248
             PReLU-6           [64, 32, 63, 47]              32
            Conv2d-7           [64, 32, 63, 47]           9,248
             PReLU-8           [64, 32, 63, 47]              32
            Conv2d-9           [64, 32, 63, 47]           9,248
            PReLU-10           [64, 32, 63, 47]              32
           Conv2d-11           [64, 32, 63, 47]           9,248
            PReLU-12           [64, 32, 63, 47]              32
           Conv2d-13           [64, 64, 63, 47]           2,112
            PReLU-14           [64, 64, 63, 47]              64
  ConvTranspose2d-15            [64, 1, 63, 47]           5,185
================================================================
Total params: 48,321
Trainable params: 48,321
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.72
Forward/backward pass size (MB): 834.23
Params size (MB): 0.18
Estimated Total Size (MB): 835.13
----------------------------------------------------------------

```

网络结果如下：

```json
Loss: MSELoss()

epoch: 1/5: 100%|████████████████████████████████████████████████| 21888/21888 [03:38<00:00, 103.12it/s, loss=0.016528]
eval psnr: 27.64
epoch: 2/5: 100%|████████████████████████████████████████████████| 21888/21888 [03:38<00:00, 105.86it/s, loss=0.003207]
eval psnr: 29.45
epoch: 3/5: 100%|████████████████████████████████████████████████| 21888/21888 [03:40<00:00, 102.57it/s, loss=0.002299]
eval psnr: 30.33
epoch: 4/5: 100%|████████████████████████████████████████████████| 21888/21888 [03:39<00:00, 110.18it/s, loss=0.001966]
eval psnr: 30.77
epoch: 5/5: 100%|████████████████████████████████████████████████| 21888/21888 [03:42<00:00, 106.32it/s, loss=0.001849]
eval psnr: 30.88

Loss: L1Loss()

epoch: 1/5: 100%|████████████████████████████████████████████████| 21888/21888 [03:41<00:00, 110.90it/s, loss=0.052537]
eval psnr: 31.27
epoch: 2/5: 100%|████████████████████████████████████████████████| 21888/21888 [03:41<00:00, 100.73it/s, loss=0.024061]
eval psnr: 32.83
epoch: 3/5: 100%|████████████████████████████████████████████████| 21888/21888 [03:42<00:00, 109.55it/s, loss=0.020765]
eval psnr: 33.01
epoch: 4/5: 100%|████████████████████████████████████████████████| 21888/21888 [03:43<00:00, 109.11it/s, loss=0.019343]
eval psnr: 33.72
epoch: 5/5: 100%|████████████████████████████████████████████████| 21888/21888 [03:44<00:00, 100.96it/s, loss=0.018861]
eval psnr: 33.84
```

提高了精度，但是训练时间大幅增加，得不偿失

# SRGAN系列

训练数据为BSDS200，测试数据为BSDS100

评价指标为PSNR

实验环境：Ubuntu20.04 LTS、pytorch 1.7.0、Colab GPU

超参数：同上

## SRResNet

网络结构如下：

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [64, 64, 25, 25]          15,616
         LeakyReLU-2           [64, 64, 25, 25]               0
            Conv2d-3           [64, 64, 25, 25]          36,864
    InstanceNorm2d-4           [64, 64, 25, 25]             128
         LeakyReLU-5           [64, 64, 25, 25]               0
            Conv2d-6           [64, 64, 25, 25]          36,864
    InstanceNorm2d-7           [64, 64, 25, 25]             128
     ResidualBlock-8           [64, 64, 25, 25]               0
            Conv2d-9           [64, 64, 25, 25]          36,864
   InstanceNorm2d-10           [64, 64, 25, 25]             128
        LeakyReLU-11           [64, 64, 25, 25]               0
           Conv2d-12           [64, 64, 25, 25]          36,864
   InstanceNorm2d-13           [64, 64, 25, 25]             128
    ResidualBlock-14           [64, 64, 25, 25]               0
           Conv2d-15           [64, 64, 25, 25]          36,864
   InstanceNorm2d-16           [64, 64, 25, 25]             128
        LeakyReLU-17           [64, 64, 25, 25]               0
           Conv2d-18           [64, 64, 25, 25]          36,864
   InstanceNorm2d-19           [64, 64, 25, 25]             128
    ResidualBlock-20           [64, 64, 25, 25]               0
           Conv2d-21           [64, 64, 25, 25]          36,864
   InstanceNorm2d-22           [64, 64, 25, 25]             128
        LeakyReLU-23           [64, 64, 25, 25]               0
           Conv2d-24           [64, 64, 25, 25]          36,864
   InstanceNorm2d-25           [64, 64, 25, 25]             128
    ResidualBlock-26           [64, 64, 25, 25]               0
           Conv2d-27           [64, 64, 25, 25]          36,864
   InstanceNorm2d-28           [64, 64, 25, 25]             128
        LeakyReLU-29           [64, 64, 25, 25]               0
           Conv2d-30           [64, 64, 25, 25]          36,864
   InstanceNorm2d-31           [64, 64, 25, 25]             128
    ResidualBlock-32           [64, 64, 25, 25]               0
           Conv2d-33           [64, 64, 25, 25]          36,864
   InstanceNorm2d-34           [64, 64, 25, 25]             128
        LeakyReLU-35           [64, 64, 25, 25]               0
           Conv2d-36           [64, 64, 25, 25]          36,864
   InstanceNorm2d-37           [64, 64, 25, 25]             128
    ResidualBlock-38           [64, 64, 25, 25]               0
           Conv2d-39           [64, 64, 25, 25]          36,864
   InstanceNorm2d-40           [64, 64, 25, 25]             128
        LeakyReLU-41           [64, 64, 25, 25]               0
           Conv2d-42           [64, 64, 25, 25]          36,864
   InstanceNorm2d-43           [64, 64, 25, 25]             128
    ResidualBlock-44           [64, 64, 25, 25]               0
           Conv2d-45           [64, 64, 25, 25]          36,864
   InstanceNorm2d-46           [64, 64, 25, 25]             128
        LeakyReLU-47           [64, 64, 25, 25]               0
           Conv2d-48           [64, 64, 25, 25]          36,864
   InstanceNorm2d-49           [64, 64, 25, 25]             128
    ResidualBlock-50           [64, 64, 25, 25]               0
           Conv2d-51           [64, 64, 25, 25]          36,864
   InstanceNorm2d-52           [64, 64, 25, 25]             128
        LeakyReLU-53           [64, 64, 25, 25]               0
           Conv2d-54           [64, 64, 25, 25]          36,864
   InstanceNorm2d-55           [64, 64, 25, 25]             128
    ResidualBlock-56           [64, 64, 25, 25]               0
           Conv2d-57           [64, 64, 25, 25]          36,864
   InstanceNorm2d-58           [64, 64, 25, 25]             128
        LeakyReLU-59           [64, 64, 25, 25]               0
           Conv2d-60           [64, 64, 25, 25]          36,864
   InstanceNorm2d-61           [64, 64, 25, 25]             128
    ResidualBlock-62           [64, 64, 25, 25]               0
           Conv2d-63           [64, 64, 25, 25]          36,864
   InstanceNorm2d-64           [64, 64, 25, 25]             128
        LeakyReLU-65           [64, 64, 25, 25]               0
           Conv2d-66           [64, 64, 25, 25]          36,864
   InstanceNorm2d-67           [64, 64, 25, 25]             128
    ResidualBlock-68           [64, 64, 25, 25]               0
           Conv2d-69           [64, 64, 25, 25]          36,864
   InstanceNorm2d-70           [64, 64, 25, 25]             128
        LeakyReLU-71           [64, 64, 25, 25]               0
           Conv2d-72           [64, 64, 25, 25]          36,864
   InstanceNorm2d-73           [64, 64, 25, 25]             128
    ResidualBlock-74           [64, 64, 25, 25]               0
           Conv2d-75           [64, 64, 25, 25]          36,864
   InstanceNorm2d-76           [64, 64, 25, 25]             128
        LeakyReLU-77           [64, 64, 25, 25]               0
           Conv2d-78           [64, 64, 25, 25]          36,864
   InstanceNorm2d-79           [64, 64, 25, 25]             128
    ResidualBlock-80           [64, 64, 25, 25]               0
           Conv2d-81           [64, 64, 25, 25]          36,864
   InstanceNorm2d-82           [64, 64, 25, 25]             128
        LeakyReLU-83           [64, 64, 25, 25]               0
           Conv2d-84           [64, 64, 25, 25]          36,864
   InstanceNorm2d-85           [64, 64, 25, 25]             128
    ResidualBlock-86           [64, 64, 25, 25]               0
           Conv2d-87           [64, 64, 25, 25]          36,864
   InstanceNorm2d-88           [64, 64, 25, 25]             128
        LeakyReLU-89           [64, 64, 25, 25]               0
           Conv2d-90           [64, 64, 25, 25]          36,864
   InstanceNorm2d-91           [64, 64, 25, 25]             128
    ResidualBlock-92           [64, 64, 25, 25]               0
           Conv2d-93           [64, 64, 25, 25]          36,864
   InstanceNorm2d-94           [64, 64, 25, 25]             128
        LeakyReLU-95           [64, 64, 25, 25]               0
           Conv2d-96           [64, 64, 25, 25]          36,864
   InstanceNorm2d-97           [64, 64, 25, 25]             128
    ResidualBlock-98           [64, 64, 25, 25]               0
           Conv2d-99           [64, 64, 25, 25]          36,928
  InstanceNorm2d-100           [64, 64, 25, 25]             128
          Conv2d-101          [64, 256, 25, 25]         147,456
    PixelShuffle-102           [64, 64, 50, 50]               0
       LeakyReLU-103           [64, 64, 50, 50]               0
          Conv2d-104          [64, 256, 50, 50]         147,456
    PixelShuffle-105         [64, 64, 100, 100]               0
       LeakyReLU-106         [64, 64, 100, 100]               0
          Conv2d-107          [64, 3, 100, 100]          15,555
================================================================
Total params: 1,546,883
Trainable params: 1,546,883
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.46
Forward/backward pass size (MB): 3139.65
Params size (MB): 5.90
Estimated Total Size (MB): 3146.01
----------------------------------------------------------------

```

网络结果如下：

```json
Loss: MSELoss()

epoch: 1/3: 100% 15552/15552 [00:46<00:00, 337.81it/s, loss=0.101984]
eval psnr: 16.67
epoch: 2/3: 100% 15552/15552 [00:44<00:00, 351.83it/s, loss=0.021455]
eval psnr: 18.51
epoch: 3/3: 100% 15552/15552 [00:44<00:00, 352.92it/s, loss=0.016832]
eval psnr: 19.04

Loss: L1Loss()

epoch: 1/3: 100% 15552/15552 [00:44<00:00, 352.23it/s, loss=0.169221]
eval psnr: 19.62
epoch: 2/3: 100% 15552/15552 [00:44<00:00, 352.25it/s, loss=0.075394]
eval psnr: 21.36
epoch: 3/3: 100% 15552/15552 [00:44<00:00, 352.06it/s, loss=0.066012]
eval psnr: 21.90
```

与FSRCNN网络结果作对比：

```json
Loss: MSELoss()

epoch: 1/3: 100% 15552/15552 [00:28<00:00, 551.22it/s, loss=0.059770]
eval psnr: 15.12
epoch: 2/3: 100% 15552/15552 [00:27<00:00, 561.50it/s, loss=0.029729]
eval psnr: 17.20
epoch: 3/3: 100% 15552/15552 [00:27<00:00, 569.08it/s, loss=0.023273]
eval psnr: 17.57

Loss: L1Loss()

epoch: 1/3: 100% 15552/15552 [00:27<00:00, 574.04it/s, loss=0.136845]
eval psnr: 17.92
epoch: 2/3: 100% 15552/15552 [00:27<00:00, 571.72it/s, loss=0.100386]
eval psnr: 18.78
epoch: 3/3: 100% 15552/15552 [00:26<00:00, 576.78it/s, loss=0.093093]
eval psnr: 19.16
```

SRResNet比FSRCNN的结果要优秀一些

## 对比实验（SRResNet）

SRResNet复现结果并不理想，其在前三个epoch与SRCNN结果相差不大，考虑到SRResNet中使用了一定的归一化（IN），我们做以下两个对比实验：

1.  SRResNet与SRCNN在相同实验环境下多次训练后的结果比较（略，SRCNN在该数据集下与SRResNet相差不大）
2.  SRResNet中的归一化操作

### 网络比较

### 归一化操作

我们可以看到IN在网络中存在与两处，一处在相近层之间加入IN操作，一处为在相远层之间加入IN操作

下面作对比：

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [64, 64, 25, 25]          15,616
         LeakyReLU-2           [64, 64, 25, 25]               0
            Conv2d-3           [64, 64, 25, 25]          36,864
         LeakyReLU-4           [64, 64, 25, 25]               0
            Conv2d-5           [64, 64, 25, 25]          36,864
     ResidualBlock-6           [64, 64, 25, 25]               0
            Conv2d-7           [64, 64, 25, 25]          36,864
         LeakyReLU-8           [64, 64, 25, 25]               0
            Conv2d-9           [64, 64, 25, 25]          36,864
    ResidualBlock-10           [64, 64, 25, 25]               0
           Conv2d-11           [64, 64, 25, 25]          36,864
        LeakyReLU-12           [64, 64, 25, 25]               0
           Conv2d-13           [64, 64, 25, 25]          36,864
    ResidualBlock-14           [64, 64, 25, 25]               0
           Conv2d-15           [64, 64, 25, 25]          36,864
        LeakyReLU-16           [64, 64, 25, 25]               0
           Conv2d-17           [64, 64, 25, 25]          36,864
    ResidualBlock-18           [64, 64, 25, 25]               0
           Conv2d-19           [64, 64, 25, 25]          36,864
        LeakyReLU-20           [64, 64, 25, 25]               0
           Conv2d-21           [64, 64, 25, 25]          36,864
    ResidualBlock-22           [64, 64, 25, 25]               0
           Conv2d-23           [64, 64, 25, 25]          36,864
        LeakyReLU-24           [64, 64, 25, 25]               0
           Conv2d-25           [64, 64, 25, 25]          36,864
    ResidualBlock-26           [64, 64, 25, 25]               0
           Conv2d-27           [64, 64, 25, 25]          36,864
        LeakyReLU-28           [64, 64, 25, 25]               0
           Conv2d-29           [64, 64, 25, 25]          36,864
    ResidualBlock-30           [64, 64, 25, 25]               0
           Conv2d-31           [64, 64, 25, 25]          36,864
        LeakyReLU-32           [64, 64, 25, 25]               0
           Conv2d-33           [64, 64, 25, 25]          36,864
    ResidualBlock-34           [64, 64, 25, 25]               0
           Conv2d-35           [64, 64, 25, 25]          36,864
        LeakyReLU-36           [64, 64, 25, 25]               0
           Conv2d-37           [64, 64, 25, 25]          36,864
    ResidualBlock-38           [64, 64, 25, 25]               0
           Conv2d-39           [64, 64, 25, 25]          36,864
        LeakyReLU-40           [64, 64, 25, 25]               0
           Conv2d-41           [64, 64, 25, 25]          36,864
    ResidualBlock-42           [64, 64, 25, 25]               0
           Conv2d-43           [64, 64, 25, 25]          36,864
        LeakyReLU-44           [64, 64, 25, 25]               0
           Conv2d-45           [64, 64, 25, 25]          36,864
    ResidualBlock-46           [64, 64, 25, 25]               0
           Conv2d-47           [64, 64, 25, 25]          36,864
        LeakyReLU-48           [64, 64, 25, 25]               0
           Conv2d-49           [64, 64, 25, 25]          36,864
    ResidualBlock-50           [64, 64, 25, 25]               0
           Conv2d-51           [64, 64, 25, 25]          36,864
        LeakyReLU-52           [64, 64, 25, 25]               0
           Conv2d-53           [64, 64, 25, 25]          36,864
    ResidualBlock-54           [64, 64, 25, 25]               0
           Conv2d-55           [64, 64, 25, 25]          36,864
        LeakyReLU-56           [64, 64, 25, 25]               0
           Conv2d-57           [64, 64, 25, 25]          36,864
    ResidualBlock-58           [64, 64, 25, 25]               0
           Conv2d-59           [64, 64, 25, 25]          36,864
        LeakyReLU-60           [64, 64, 25, 25]               0
           Conv2d-61           [64, 64, 25, 25]          36,864
    ResidualBlock-62           [64, 64, 25, 25]               0
           Conv2d-63           [64, 64, 25, 25]          36,864
        LeakyReLU-64           [64, 64, 25, 25]               0
           Conv2d-65           [64, 64, 25, 25]          36,864
    ResidualBlock-66           [64, 64, 25, 25]               0
           Conv2d-67           [64, 64, 25, 25]          36,928
           Conv2d-68          [64, 256, 25, 25]         147,456
     PixelShuffle-69           [64, 64, 50, 50]               0
        LeakyReLU-70           [64, 64, 50, 50]               0
           Conv2d-71          [64, 256, 50, 50]         147,456
     PixelShuffle-72         [64, 64, 100, 100]               0
        LeakyReLU-73         [64, 64, 100, 100]               0
           Conv2d-74          [64, 3, 100, 100]          15,555
================================================================
Total params: 1,542,659
Trainable params: 1,542,659
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.46
Forward/backward pass size (MB): 2495.12
Params size (MB): 5.88
Estimated Total Size (MB): 2501.46
----------------------------------------------------------------

Loss: MSELoss()

epoch: 1/3: 100% 15552/15552 [00:38<00:00, 405.63it/s, loss=104045.110191]
eval psnr: 5.06
epoch: 2/3: 100% 15552/15552 [00:38<00:00, 408.59it/s, loss=2996.520365]
eval psnr: 5.07
epoch: 3/3: 100% 15552/15552 [00:38<00:00, 407.24it/s, loss=1897.019418]
eval psnr: 5.07

Loss: L1Loss()

epoch: 1/3: 100% 15552/15552 [00:38<00:00, 408.17it/s, loss=87.379543]
eval psnr: 5.30
epoch: 2/3: 100% 15552/15552 [00:38<00:00, 407.57it/s, loss=4.519954]
eval psnr: 5.55
epoch: 3/3: 100% 15552/15552 [00:38<00:00, 407.03it/s, loss=2.820541]
eval psnr: 5.66

```

可以看到去除ResBlock之间的IN层之后，网络仍可以收敛但是性能极差

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [64, 64, 25, 25]          15,616
         LeakyReLU-2           [64, 64, 25, 25]               0
            Conv2d-3           [64, 64, 25, 25]          36,864
         LeakyReLU-4           [64, 64, 25, 25]               0
            Conv2d-5           [64, 64, 25, 25]          36,864
     ResidualBlock-6           [64, 64, 25, 25]               0
            Conv2d-7           [64, 64, 25, 25]          36,864
         LeakyReLU-8           [64, 64, 25, 25]               0
            Conv2d-9           [64, 64, 25, 25]          36,864
    ResidualBlock-10           [64, 64, 25, 25]               0
           Conv2d-11           [64, 64, 25, 25]          36,864
        LeakyReLU-12           [64, 64, 25, 25]               0
           Conv2d-13           [64, 64, 25, 25]          36,864
    ResidualBlock-14           [64, 64, 25, 25]               0
           Conv2d-15           [64, 64, 25, 25]          36,864
        LeakyReLU-16           [64, 64, 25, 25]               0
           Conv2d-17           [64, 64, 25, 25]          36,864
    ResidualBlock-18           [64, 64, 25, 25]               0
           Conv2d-19           [64, 64, 25, 25]          36,864
        LeakyReLU-20           [64, 64, 25, 25]               0
           Conv2d-21           [64, 64, 25, 25]          36,864
    ResidualBlock-22           [64, 64, 25, 25]               0
           Conv2d-23           [64, 64, 25, 25]          36,864
        LeakyReLU-24           [64, 64, 25, 25]               0
           Conv2d-25           [64, 64, 25, 25]          36,864
    ResidualBlock-26           [64, 64, 25, 25]               0
           Conv2d-27           [64, 64, 25, 25]          36,864
        LeakyReLU-28           [64, 64, 25, 25]               0
           Conv2d-29           [64, 64, 25, 25]          36,864
    ResidualBlock-30           [64, 64, 25, 25]               0
           Conv2d-31           [64, 64, 25, 25]          36,864
        LeakyReLU-32           [64, 64, 25, 25]               0
           Conv2d-33           [64, 64, 25, 25]          36,864
    ResidualBlock-34           [64, 64, 25, 25]               0
           Conv2d-35           [64, 64, 25, 25]          36,864
        LeakyReLU-36           [64, 64, 25, 25]               0
           Conv2d-37           [64, 64, 25, 25]          36,864
    ResidualBlock-38           [64, 64, 25, 25]               0
           Conv2d-39           [64, 64, 25, 25]          36,864
        LeakyReLU-40           [64, 64, 25, 25]               0
           Conv2d-41           [64, 64, 25, 25]          36,864
    ResidualBlock-42           [64, 64, 25, 25]               0
           Conv2d-43           [64, 64, 25, 25]          36,864
        LeakyReLU-44           [64, 64, 25, 25]               0
           Conv2d-45           [64, 64, 25, 25]          36,864
    ResidualBlock-46           [64, 64, 25, 25]               0
           Conv2d-47           [64, 64, 25, 25]          36,864
        LeakyReLU-48           [64, 64, 25, 25]               0
           Conv2d-49           [64, 64, 25, 25]          36,864
    ResidualBlock-50           [64, 64, 25, 25]               0
           Conv2d-51           [64, 64, 25, 25]          36,864
        LeakyReLU-52           [64, 64, 25, 25]               0
           Conv2d-53           [64, 64, 25, 25]          36,864
    ResidualBlock-54           [64, 64, 25, 25]               0
           Conv2d-55           [64, 64, 25, 25]          36,864
        LeakyReLU-56           [64, 64, 25, 25]               0
           Conv2d-57           [64, 64, 25, 25]          36,864
    ResidualBlock-58           [64, 64, 25, 25]               0
           Conv2d-59           [64, 64, 25, 25]          36,864
        LeakyReLU-60           [64, 64, 25, 25]               0
           Conv2d-61           [64, 64, 25, 25]          36,864
    ResidualBlock-62           [64, 64, 25, 25]               0
           Conv2d-63           [64, 64, 25, 25]          36,864
        LeakyReLU-64           [64, 64, 25, 25]               0
           Conv2d-65           [64, 64, 25, 25]          36,864
    ResidualBlock-66           [64, 64, 25, 25]               0
           Conv2d-67           [64, 64, 25, 25]          36,928
   InstanceNorm2d-68           [64, 64, 25, 25]             128
           Conv2d-69          [64, 256, 25, 25]         147,456
     PixelShuffle-70           [64, 64, 50, 50]               0
        LeakyReLU-71           [64, 64, 50, 50]               0
           Conv2d-72          [64, 256, 50, 50]         147,456
     PixelShuffle-73         [64, 64, 100, 100]               0
        LeakyReLU-74         [64, 64, 100, 100]               0
           Conv2d-75          [64, 3, 100, 100]          15,555
================================================================
Total params: 1,542,787
Trainable params: 1,542,787
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.46
Forward/backward pass size (MB): 2514.65
Params size (MB): 5.89
Estimated Total Size (MB): 2520.99
----------------------------------------------------------------

Loss: MSELoss()

epoch: 1/3: 100% 15552/15552 [00:38<00:00, 406.25it/s, loss=0.044377]
eval psnr: 18.90
epoch: 2/3: 100% 15552/15552 [00:38<00:00, 408.41it/s, loss=0.014153]
eval psnr: 20.53
epoch: 3/3: 100% 15552/15552 [00:38<00:00, 407.92it/s, loss=0.011625]
eval psnr: 20.94

Loss: L1Loss()

epoch: 1/3: 100% 15552/15552 [00:38<00:00, 406.15it/s, loss=0.115289]
eval psnr: 21.05
epoch: 2/3: 100% 15552/15552 [00:38<00:00, 407.70it/s, loss=0.065979]
eval psnr: 22.22
epoch: 3/3: 100% 15552/15552 [00:38<00:00, 407.70it/s, loss=0.059689]
eval psnr: 22.63

```

我们仅去除相近层之间的IN操作后，发现该网络性能相较原始有一定的提升

解释：我还没找到、想到相关解释（TODO）

## SRGAN

GAN，拿VGG类做了判别，Loss在L1Loss基础上增加了判别网络的Loss

> 网络模型较为复杂，在代码中有显示

新增超参（多次试验后此为最佳选项）：

1. 两种loss之和：`total_loss = content_loss + 1e-3 * adversarial_loss`
2. 学习率为1e-3
3. batch_size改为128

网络结构，见`modelviz.pdf`

网络结果（PSNR与SSIM指标在GN前后相差不大，PSNR在25.2上下浮动，SSIM在0.7610上下浮动）

原图：

![原图](./test.jpg)

pretrain结果：`psnr: 23.7909 ssim:0.7621`

![pretrain](./srresnet_pretrain.jpg)

gan结果： `psnr: 23.8192 ssim:0.7632`

![gan](./srgan_final.jpg)

很明显GAN生成的更真实一点（虽然效果不咋地，数据集太小了）

> SRGAN完整训练环境为ubuntu20.04LTS，RTX3090，数据集仅18000张，15488作为train


